---
title: "Carseats R Notebook"
output:
  html_notebook:
    code_folding: none
    highlight: pygments
    theme: sandstone
editor_options:
  chunk_output_type: inline
---
Execute: *Cmd+Shift+Enter*. 
New Chunk: *Cmd+Option+I*.
Preview: *Cmd+Shift+K*
Notes: The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

## Configuration

```{r}
library(leaps)
library(modelr)
library(mgcv)
library(ggplot2)
library(DataExplorer)
library(dplyr)
library(GGally)
library(ggExtra)
library(caret)
library(reshape2)
library(RColorBrewer)
library(plotly)
library(corrplot)
```

## System Information
Due to the large number of libraries in use I have provided system information.

```{r}
sessionInfo()
sapply(c('repr', 'IRdisplay', 'IRkernel'), function(p) paste(packageVersion(p)))
```

# Data
First we inspect the raw records using `bash`: 

```{bash}
head -n 5 data/Carseats_org.csv
```

Now I load the data into `r`, and drop the "ID" column. 

```{r}
carseats <- read.csv("data/Carseats_org.csv", header = T, stringsAsFactors = T)

drops <- c("X")
carseats <- carseats[ , !(names(carseats) %in% drops)]
head(carseats, 10)
```

Here I create two new dataframes to manage numeric and categorical data.

```{r}
# get vectors of continuous and categorical cols
nums <- dplyr::select_if(carseats, is.numeric)
cats <- dplyr::select_if(carseats, is.factor)

nums[sample(nrow(nums), 10), ]

cats[sample(nrow(cats), 10), ]
```

Let's get quick summaries of each:

```{r}
summary(nums)
```

```{r}
summary(cats)
```

```{r}
str(nums)
str(cats)
```


## Data Dimensionality
This command is to inspect the different data types in the data. 

```{r}
str(carseats)
print("")
print(paste('Number of Columns:', ncol(carseats)))
print(paste('Number of Numeric Columns:', ncol(nums)))
print(paste('Number of Categorical Columns:', ncol(cats)))
```

Here's a quick way to examine general properties of the data:

```{r}
DataExplorer::introduce(data=carseats)
```

# Numeric Plotting
I start out with a few general scatter plots.

```{r}
plot_ly(data=carseats, 
        x=~Age, 
        y=~Sales, 
        mode = 'markers', 
        type = 'scatter', 
        color=~ShelveLoc) %>%
            layout(title = "Age, Shelf Location, and Sales Scatter Plot", width=900) 
```

This plot below shows good separation and a weak linear trend. These variables are worth investigating.

```{r}
plot_ly(data=carseats, 
        x=~Price, 
        y=~Sales, 
        mode = 'markers', 
        type = 'scatter', 
        color=~ShelveLoc) %>%
            layout(title = "Price, Shelf Location, and Sales Scatter Plot", width=900)
```

Here we inspect the density of the 'Price vs Sales' relationship:

```{r}
plot_ly(data=carseats, 
        x=~Price, 
        y=~Sales, 
        mode = 'markers', 
        size = ~Price,
        type = 'scatter',
        colors = "Dark2",
        alpha = .6) %>%
            layout(title = "Price, US, and Sales Scatter Plot", width=900) 
```

# Normalization
I choose to normalize the numeric data in order to be able to plot each variable on the same scale. This will allow me to investigate the variation of each predictor relaticve to Sales.

```{r}
preObj <- preProcess(nums, method=c("center", "scale"))
scaled.nums <- predict(preObj, nums)

head(scaled.nums)
```

```{r}
str(scaled.nums)
print("")
summary(scaled.nums)
```

## Distributions
Here are scaled distributions (histograms and density plots) for each numeric variable, including Sales. The variables relating to money ($) tend to be approximately normal. Many other variables tend to be approximately uniform, which does not bode well for their predictive power.

```{r}
scaled.nums %>%
    tidyr::gather() %>% 
        ggplot(aes(x=value,y=..density..))+

            ggtitle('Distributions of Continous Variables (scaled)') +

            facet_wrap(~ key, scales = "free") +
            geom_histogram(fill=I("orange"), col=I("black")) +

            facet_wrap(~ key, scales = "free") +
            geom_density(color="blue", fill='light blue', alpha = 0.4)
```

Here we plot all numeric variables against their distributions. This is just another way to examine the information shown above.

```{r}
scaled.nums %>%
    tidyr::gather() %>% 
        plot_ly(x=~key, y=~value,
                type = "box", 
                boxpoints = "all", 
                jitter = 0.4,
                pointpos = 0,
                color = ~key,
                colors = "Dark2") %>%
                      subplot(shareX = TRUE)  %>%
                            layout(title = "Numeric Variable Distributions (scaled)", 
                                  yaxis=list(title='Standard Deviation'),
                                  xaxis=list(title='Variable'),
                                  autosize=FALSE,
                                  width=900,
                                  height=900)
```

## Scatterplots

Here we plot all numeric variables against Sales (scaled). This allows us to investigate possible linear relationships between that variable and Sales. As shown below, only 'Price' appears to have a linear relationshop worth investigating.

```{r}
scaled.nums %>%
    tidyr::gather(-Sales, key = "var", value = "value") %>%
    split(.$var) %>%
       lapply(function(d) plot_ly(d, x=~value, y=~Sales, 
                mode = 'markers', 
                type = 'scatter',
                colors = 'Set3',
                size = ~Sales^2,
                marker = list(line = list(
                color = 'black',
                width = 2)),
                name=~var)) %>%
                      subplot(nrows=7, shareY=TRUE)  %>%
                            layout(title = "Scatterplot Numeric vs Sales (scaled)<br>Size=Sales^2",
                                   autosize=FALSE,
                                   width=900,
                                   height=1500,
                                   yaxis=list(title='Sales'))

```

By adding naive regression lines to a few scatterplots we can confirm our suspicions:

```{r}
fit.Pop <- lm(Sales ~ Population, data = scaled.nums)
fit.Age <- lm(Sales ~ Age, data = scaled.nums)
fit.CompPrice <- lm(Sales ~ CompPrice, data = scaled.nums)
fit.Price <- lm(Sales ~ Price, data = scaled.nums)

p1 <-  plot_ly(scaled.nums, 
               x = ~Population,
               name = 'Population vs Sales Regression Line') %>% 
                   add_markers(y = ~Sales,
                               name = 'Population vs Sales Observations') %>% 
                                    add_lines(x = ~Population, 
                                              y = fitted(fit.Pop)) 
p2 <-  plot_ly(scaled.nums, 
               x = ~Age,
               name = 'Age vs Sales Regression Line') %>% 
                   add_markers(y = ~Sales,
                               name = 'Age vs Sales Observations') %>% 
                                    add_lines(x = ~Age, 
                                              y = fitted(fit.Age)) 

p3 <-  plot_ly(scaled.nums, 
               x = ~CompPrice,
               name = 'CompPrice vs Sales Regression Line') %>% 
                   add_markers(y = ~Sales,
                               name = 'CompPrice vs Sales Observations') %>% 
                                    add_lines(x = ~CompPrice, 
                                              y = fitted(fit.CompPrice)) 

p4 <-  plot_ly(scaled.nums, 
               x = ~Price,
               name = 'Price vs Sales Regression Line') %>% 
                   add_markers(y = ~Sales,
                               name = 'Price vs Sales Observations') %>% 
                                    add_lines(x = ~Price, 
                                              y = fitted(fit.Price)) 

subplot(p1, p2, p3, p4, nrows=2, shareY=TRUE) %>%
    layout(title = "Scatterplot Numeric vs Sales (scaled) <br> With Regression Lines",
           autosize=FALSE,
           width=1000,
           height=700,
           yaxis=list(title='Sales'))

```

Let's compare the slopes:

```{r}
scaled.nums %>%
    plot_ly(y = ~Sales) %>%
      add_lines(x= ~Population, y = fitted(fit.Pop), 
                name = "fit.Pop slope", line = list(shape = "linear")) %>%
      add_lines(x= ~Age, y = fitted(fit.Age), 
                name = "fit.Age slope", line = list(shape = "linear")) %>%
      add_lines(x= ~CompPrice, y = fitted(fit.CompPrice), 
                name = "fit.CompPrice slope", line = list(shape = "linear")) %>%
      add_lines(x= ~Price, y = fitted(fit.Price), 
                name = "fit.Price slope", line = list(shape = "linear")) %>%
            
    layout(title = "Regression Lines vs Sales (scaled)",
           autosize=FALSE,
           width=700,
           height=500,
           yaxis=list(title='Sales'),
           xaxis=list(title='Scaled Numeric Variable'))
```

Here's a pretty graphic that doesn't help me understand anything about the data.

```{r}
y = scaled.nums$Sales
x = scaled.nums$Price

s <- subplot(
  plot_ly(x = x, color = I("black"), type = 'histogram'), 
  plotly_empty(), 
  plot_ly(x = x, y = y, type = 'histogram2dcontour', showscale = F), 
  plot_ly(y = y, color = I("black"), type = 'histogram'),
  nrows = 2, heights = c(0.2, 0.8), widths = c(0.8, 0.2),
  shareX = TRUE, shareY = TRUE, titleX = FALSE, titleY = FALSE)

layout(s, showlegend = FALSE, autosize=FALSE,
           width=700,
           height=500, 
           yaxis=list(title='Sales'),
           xaxis=list(title='Price'))
```

# Categorical Plotting
First, let's create a dataframe that we can use:
```{r}
categorical.by.sales = cbind(Sales = scaled.nums$Sales, cats)
```

Let's try 'em all at once. This works, but I don't manage the colors very well. 

```{r}
colr = brewer.pal(9, "Set1")

categorical.by.sales %>%
    tidyr::gather(-Sales, key = "var", value = "value") %>%
    split(.$var) %>%
       lapply(function(d) plot_ly(d, x=~paste(var, '<br>' , value), y=~Sales,
                type = "box", 
                boxpoints = "all", 
                jitter = .2,
                pointpos = 0,
                color =~paste(var, ":", value),
                colors = sample(colr, length(unique(d$value))))) %>%
#                 colors = "Set1")) %>%
                      subplot(shareY = TRUE, nrows=3)  %>%
                            layout(title = "Categorical Variable Distributions vs Sales (scaled)", 
                                  yaxis=list(title='Sales Standard Deviation'),
                                  xaxis=list(title=''),
                                  autosize=FALSE,
                                  width=900,
                                  height=1500)
```

I run these each separately and get much nicer plots. 

```{r}
categorical.by.sales %>%               
    plot_ly(x = ~US, y = ~Sales,
        split = ~US,
        type = 'violin',
        box = list(visible = TRUE),
        meanline = list(visible = TRUE)) %>% 
              layout(xaxis = list(title = "US"),
                     yaxis = list(title = "Sales",zeroline = FALSE))

```

```{r}
categorical.by.sales %>%               
    plot_ly(x = ~Urban, y = ~Sales,
        split = ~Urban,
        type = 'violin',
        box = list(visible = TRUE),
        meanline = list(visible = TRUE)) %>% 
              layout(xaxis = list(title = "Urban"),
                     yaxis = list(title = "Sales",zeroline = FALSE))
```

Now this is interesting. We suspected that 'ShelveLoc' would be important based on one of the early scatterplots. It seems that this is the case.

```{r}

categorical.by.sales %>%               
    plot_ly(x = ~ShelveLoc, y = ~Sales,
        split = ~ShelveLoc,
        type = 'violin',
        box = list(visible = TRUE),
        meanline = list(visible = TRUE)) %>% 
              layout(xaxis = list(title = "ShelveLoc"),
                     yaxis = list(title = "Sales",zeroline = FALSE))
```

# Linear Regression

First, let's merge the data set into a single dataframe
```{r}
scaled.merged <- merge(scaled.nums, categorical.by.sales, by="Sales")

head(scaled.merged)
```

First, let's look at some things that may give us trouble. Luckily it looks like the only serious correlation is with our dependent variable.

```{r}
res <- cor(scaled.nums)

corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

It appears that residuals are roughly symmetrical around 0. That's strange. Mostly due to a relatively poor overall fit. Note how close to zero most of the coefficient estimates are.

```{r}
simple.lm <- lm(Sales~., data=scaled.merged)
summary(simple.lm)
```

# Interaction Terms

Here we define a new model with some interaction terms: 
    a.  Income and Advertising
    b.  Income and CompPrice
    c.   Price and Age


```{r}
interaction.lm <- lm(Sales~. + Income*Advertising + Income*CompPrice + Price*Age, data=scaled.merged)
summary(interaction.lm)
```

## Variable Significance

6.  Which variables ( and interaction pairs ) are significant ?  Do you find anything surprising or counterintuitive in the results?  Describe any response variable or lack of response that seems surprising.


```{r}

```

7.  Repeat the above but using the “leaps” package to perform subset selection, using the full model using ONLY the following variables:

Price
Population
CompPrice
Income
Education
Age
Advertising
ShelveLoc

Plus the following interaction term:
Income and Age
Income and Advertising
Income and CompPrice

Use the regsubsets command in “leaps” to do the full model ( do NOT specify really.big=T as it will require a long time to complete, along with forward and backward subset selection.   Use the R file, Subset_select.R as a guide for syntax.


```{r}

```

8.  Use the following model selection criteria to determine which of the resulting models is “best” according to RSS, adjusted R squared, Mallow’s Cp, and BIC.

The result of the regressions is a model object, so you can query the proper coefficient of the model to determine which model, of the multiple models fit, produces the best metric.

Create a  table, or simply list, for the full model, and forward and backward selection, which model # is the best, then write out the model, that is put the model in the form y=intercept + coefficient*variable, for the best model using the BIC measure.  NOTE:  these may be the same model, or may differ slightly. 



```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```


