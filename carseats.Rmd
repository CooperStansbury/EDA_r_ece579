---
title: "Carseats R Notebook"
output:
  html_document
---
# Introduction
Here is my analysis of `data/Carseats_org.csv`.

## Configuration

```{r, message=FALSE}
library(leaps)
library(data.table)
library(stringr)
library(caret)
library(ggplot2)
library(DataExplorer)
library(dplyr)
library(ggExtra)
library(RColorBrewer)
library(plotly)
library(corrplot)
library(htmltools)
library(MASS)
library(GPArotation)
library(psych)
library(mgcv)
library(tidyverse)
library(modelr)
library(mltools)
```

## System Information
Due to the large number of libraries in use I have provided system information.

```{r}
sessionInfo()
sapply(c('repr', 'IRdisplay', 'IRkernel'), function(p) paste(packageVersion(p)))
```

# Data
I load the data into `r`, and drop the "ID" column. Note: before reading this data in it is generally a good idea to look at the data in 'raw' form. To do this, I typically start with something like: `head -n 5 data/Carseats_org.csv`:

```
"","Sales","CompPrice","Income","Advertising","Population","Price","ShelveLoc","Age","Education","Urban","US"
"1",9.5,138,73,11,276,120,"Bad",42,17,"Yes","Yes"
"2",11.22,111,48,16,260,83,"Good",65,10,"Yes","Yes"
"3",10.06,113,35,10,269,80,"Medium",59,12,"Yes","Yes"
"4",7.4,117,100,4,466,97,"Medium",55,14,"Yes","Yes"
```

This can be done in `.Rmd` files using a bash chunk, but it causes problems with GitHub pages, so I manually recreate it here.

Next I load the data into R Studio:

```{r}
carseats <- read.csv("data/Carseats_org.csv", header = T, stringsAsFactors = T)

drops <- c("X")
carseats <- carseats[ , !(names(carseats) %in% drops)]
head(carseats, 10)
```

I create two new data frames to manage numeric and categorical data separately. We take a peak at each data frame by sampling. This is to avoid thinking we know everything about the data from `head()` and `tail()` commands. Also, if you *don't* set the random seed you get different samples each time you run this chunk, which over time can help you understand the data.

```{r}
# get vectors of continuous and categorical cols
nums <- dplyr::select_if(carseats, is.numeric)
cats <- dplyr::select_if(carseats, is.factor)

nums[sample(nrow(nums), 10), ]

cats[sample(nrow(cats), 10), ]
```

## Numeric Summaries

```{r}
summary(nums)
```

```{r}
str(nums)
```

## Categorical Summaries

```{r}
summary(cats)
```

```{r}
str(cats)
```


# Data Dimensionality
This command is to inspect the different data types in the data. I print out the dimensionality of the data using `dim()`, `str()` and `ncol()`, all of which are built into {base} R. 

```{r}
str(carseats)
```

```{r}
print(paste('Number of Columns:', ncol(carseats)))
print(paste('Number of Numeric Columns:', ncol(nums)))
print(paste('Number of Categorical Columns:', ncol(cats)))
```


```{r}
dim(carseats)
dim(nums)
dim(cats)
```

Here's a quick way to examine general properties of the data, though I will say that it doesn't do a very good job with missing data (as demonstrated below).

```{r}
DataExplorer::introduce(data=carseats)
```

Finally, I want to look at the first and last rows of the data set. Just to be safe. This will also help me confirm that the various merges using `cbind()` are effective and don't introduce problems.

```{r}
head(carseats, 2)
tail(carseats, 2)

head(nums, 2)
tail(nums, 2)

head(cats, 2)
tail(cats, 2)
```

# Numeric Plotting
Let's start out with a few general scatter plots. The scatter plot below shows us decent stratification of Sales against Shelf Location, but is inconclusive w.r.t. Age. This makes sense, if you have a baby, it doesn't matter what age *you* are, you still need a car seat (assuming you have a car...). I'm slightly surprised that there are no data points below 25 for Age and that Sales seem to be approximately uniformly distributed all the way up to 80. This is a clue that this data is synthetic (it is).

```{r, message=FALSE}
plot_ly(data=carseats,
        x=~Age,
        y=~Sales,
        mode = 'markers',
        type = 'scatter',
        color=~ShelveLoc) %>%
            layout(title = "Age, Shelf Location vs Sales Scatter Plot", showlegend= TRUE, width=700)
```

This plot below shows good separation and a weak linear trend. These variables are worth investigating. In fact, these variables are likely to be most valuable later on.

```{r, message=FALSE}
plot_ly(data=carseats,
        x=~Price,
        y=~Sales,
        mode = 'markers',
        type = 'scatter',
        color=~ShelveLoc) %>%
            layout(title = "Price, Shelf Location vs Sales Scatter Plot", width=900)
```

Here we inspect the density of the 'Price vs Sales' relationship. No real surprises, there's more in the middle. This makes sense. Also, it looks like people don't value child safety as much as saving a few dollars. 

```{r, message=FALSE}
plot_ly(data=carseats,
        x=~Price,
        y=~Sales,
        mode = 'markers',
        size = ~Price,
        type = 'scatter',
        colors = "Dark2",
        alpha = .6) %>%
            layout(title = "Price, US vs Sales Scatter Plot", width=900)
```

# Normalization
I choose to normalize the numeric data in order to be able to plot each variable on the same scale. This will allow me to investigate the variation of each predictor relative to Sales.

```{r}
preObj <- preProcess(nums, method=c("center", "scale"))
scaled.nums <- predict(preObj, nums)

head(scaled.nums, 2)
tail(scaled.nums, 2)
```

```{r}
str(scaled.nums)
print("")
summary(scaled.nums)
```

## Distributions
Here are scaled distributions (histograms and density plots) for each numeric variable, including Sales. The variables relating to money ($) tend to be approximately normal. Many other variables tend to be approximately uniform, which does not bode well for their predictive power.

#### NOTE: When the axis ticks for a variable are approximately in the range [-3,3] they represent standard deviations around the mean (0). I have tried to include '(scaled)' in all plot titles when this is the case, but I bet I missed one or two.

```{r, message=FALSE}
scaled.nums %>%
    tidyr::gather() %>%
        ggplot(aes(x=value,y=..density..))+

            ggtitle('Distributions of Continous Variables (scaled)') +

            facet_wrap(~ key, scales = "free") +
            geom_histogram(fill=I("orange"), col=I("black"), bins = 50) +

            facet_wrap(~ key, scales = "free") +
            geom_density(color="blue", fill='light blue', alpha = 0.4)
```

Here we plot all numeric variables against their distributions. This is just another way to examine the information shown above. Note again that many variables seem to be generate from a uniform distribution.

```{r, message=FALSE}
scaled.nums %>%
    tidyr::gather() %>%
        plot_ly(x=~key, y=~value,
                type = "box",
                boxpoints = "all",
                jitter = 0.4,
                pointpos = 0,
                color = ~key,
                colors = "Dark2") %>%
                      subplot(shareX = TRUE)  %>%
                            layout(title = "Numeric Variable Distributions (scaled)",
                                  yaxis=list(title='Standard Deviation'),
                                  xaxis=list(title='Variable'),
                                  autosize=FALSE,
                                  width=900,
                                  height=500)
```

## Scatterplots

Here we plot all numeric variables against Sales (scaled). This allows us to investigate possible linear relationships between that variable and Sales. As shown below, only 'Price' appears to have a linear relationship worth investigating. This took me so long to figure out.

```{r, message=FALSE}

numeric.scatterplots <- htmltools::tagList()
count = 1

for (i in names(scaled.nums[,-1])) {

  numeric.scatterplots[[count]] <- plot_ly(scaled.nums, x=scaled.nums[,i], y=scaled.nums$Sales,
                    colors = 'RdYlGn',
                    mode = 'markers',
                    type = 'scatter',
                    size = scaled.nums$Sales^2,
                    color = scaled.nums$Sales,
                    marker = list(line = list(color = 'black',width = 2)),
                    name=paste(i)) %>%
          layout(title = paste(i, "vs Sales (scaled)<br>Size=Sales^2"),
                               yaxis=list(title='Sales'),
                               xaxis=list(title=i),
                               showlegend = FALSE)

  count = count + 1

}

numeric.scatterplots

```

By adding naive regression lines to a few scatter plots we can confirm our suspicions. Ordinarily I would investigate a few other possible fits, perhaps using something like `gam()`, but these variables show no signs of clear relationships. 

```{r, message=FALSE}
fit.Pop <- lm(Sales ~ Population, data = scaled.nums)
fit.Age <- lm(Sales ~ Age, data = scaled.nums)
fit.CompPrice <- lm(Sales ~ CompPrice, data = scaled.nums)
fit.Price <- lm(Sales ~ Price, data = scaled.nums)

regression.scatterplots <- htmltools::tagList()

regression.scatterplots[[1]] <-  plot_ly(scaled.nums,
          x = ~Population,
          name = 'Population vs Sales Regression Line') %>%
              add_markers(y = ~Sales,
                    name = 'Population vs Sales Observations') %>%
                            add_lines(x = ~Population,
                                  y = fitted(fit.Pop)) %>%
                                        layout(title = "Population vs Sales",
                                               yaxis=list(title='Sales',
                                               xaxis=list(title='Population')),
                                               showlegend = FALSE)



regression.scatterplots[[2]] <-  plot_ly(scaled.nums,
          x = ~Age,
          name = 'Age vs Sales Regression Line') %>%
              add_markers(y = ~Sales,
                    name = 'Age vs Sales Observations') %>%
                            add_lines(x = ~Age,
                                  y = fitted(fit.Age)) %>%
                                        layout(title = "Age vs Sales",
                                               yaxis=list(title='Sales',
                                               xaxis=list(title='Age')),
                                               showlegend = FALSE)

regression.scatterplots[[3]] <-  plot_ly(scaled.nums,
          x = ~CompPrice,
          name = 'CompPrice vs Sales Regression Line') %>%
              add_markers(y = ~Sales,
                    name = 'CompPrice vs Sales Observations') %>%
                            add_lines(x = ~CompPrice,
                                  y = fitted(fit.CompPrice)) %>%
                                        layout(title = "CompPrice vs Sales",
                                               yaxis=list(title='Sales',
                                               xaxis=list(title='CompPrice')),
                                               showlegend = FALSE)

regression.scatterplots[[4]] <-  plot_ly(scaled.nums,
          x = ~Price,
          name = 'Price vs Sales Regression Line') %>%
              add_markers(y = ~Sales,
                    name = 'Price vs Sales Observations') %>%
                            add_lines(x = ~Price,
                                  y = fitted(fit.Price)) %>%
                                        layout(title = "Price vs Sales",
                                               yaxis=list(title='Sales',
                                               xaxis=list(title='Price')),
                                               showlegend = FALSE)


regression.scatterplots
```

Let's compare the slopes:

```{r, message=FALSE}
scaled.nums %>%
    plot_ly(y = ~Sales) %>%
      add_lines(x= ~Population, y = fitted(fit.Pop),
                name = "fit.Pop slope", line = list(shape = "linear")) %>%
      add_lines(x= ~Age, y = fitted(fit.Age),
                name = "fit.Age slope", line = list(shape = "linear")) %>%
      add_lines(x= ~CompPrice, y = fitted(fit.CompPrice),
                name = "fit.CompPrice slope", line = list(shape = "linear")) %>%
      add_lines(x= ~Price, y = fitted(fit.Price),
                name = "fit.Price slope", line = list(shape = "linear")) %>%

    layout(title = "Regression Lines vs Sales (scaled)",
           autosize=FALSE,
           width=900,
           yaxis=list(title='Sales'),
           xaxis=list(title='Scaled Numeric Variable'))
```

Here's a pretty graphic that doesn't help me understand anything about the data. Be warned.

```{r, message=FALSE}
y = scaled.nums$Sales
x = scaled.nums$Price

s <- subplot(
  plot_ly(x = x, color = I("black"), type = 'histogram'),
  plotly_empty(),
  plot_ly(x = x, y = y, type = 'histogram2dcontour', showscale = F),
  plot_ly(y = y, color = I("black"), type = 'histogram'),
  nrows = 2, heights = c(0.2, 0.8), widths = c(0.8, 0.2),
  shareX = TRUE, shareY = TRUE, titleX = FALSE, titleY = FALSE)

layout(s, showlegend = FALSE, autosize=FALSE,
           width=700,
           height=500,
           yaxis=list(title='Sales'),
           xaxis=list(title='Price'))
```

# Categorical Plotting
First, let's create a data frame that we can use to investigate relationships with sales:

```{r, message=FALSE}
categorical.by.sales = cbind(Sales = scaled.nums$Sales, cats)
str(categorical.by.sales)
```

Here we can see all categorical by Sales. We suspected that 'ShelveLoc' would be important based on one of the early scatter plots. It seems that this is the case. It almost seems to me like the simulated customers in this data set care only about price and are buying car seats _impulsively_. This, to me, doesn't make sense as customer behavior. I would expect many customers to have researched their purchase before hand and therefore to see a weaker influence of Shelf Location on Sales.

```{r, message=FALSE}

categorical.boxplots <- htmltools::tagList()
count = 1

for (i in names(categorical.by.sales[,-1])) {

  categorical.boxplots[[count]] <- plot_ly(categorical.by.sales, x=categorical.by.sales[,i], y=categorical.by.sales$Sales,
                        type = "box",
                        boxpoints = "all",
                        jitter = .2,
                        pointpos = 0,
                        color =categorical.by.sales[,i],
                        colors='Set1',
                        name=paste(i)) %>%
                            layout(title = paste(i, "vs Sales (scaled)"),
                                  showlegend = TRUE,
                                  yaxis=list(title='Sales Standard Deviation'),
                                  xaxis=list(title=i))

  count = count + 1

}

categorical.boxplots
```

Here's the same thing, but more musically:

```{r, message=FALSE}

categorical.violins <- htmltools::tagList()
count = 1

for (i in names(categorical.by.sales[,-1])) {

  categorical.violins[[count]] <- plot_ly(categorical.by.sales, x=categorical.by.sales[,i], y=categorical.by.sales$Sales,
                        split = categorical.by.sales[,i],
                        type = 'violin',
                        colors='Set1',
                        name=paste(i),
                        box = list(visible = TRUE),
                        meanline = list(visible = TRUE)) %>%
                            layout(xaxis = list(title = "US"),
                                yaxis = list(title = "Sales",zeroline = FALSE))


  count = count + 1

}

categorical.violins
```

# Linear Regression

First, let's merge the data set into a single data frame. We want to use the scaled numeric features and the categorical features.

```{r}

scaled.merged <- cbind(categorical.by.sales[,-1], scaled.nums)
str(scaled.merged)
```

We'll also do a sanity check:

```{r}
head(nums, 2)
tail(nums, 2)

head(scaled.merged, 2)
tail(scaled.merged, 2)
```


Now let's look at some things that may give us trouble. Luckily it looks like the only serious correlation is with our dependent variable. We'll want to watch the 'Price' vs 'CompPrice' relationship. Uninterestingly it looks like advertising may have a small influence on Sales.

```{r, message=FALSE}
res <- cor(scaled.nums)

corrplot(res, type = "upper", order = "hclust",
         tl.col = "black", tl.srt = 45)
```

## Preliminary Regression

The first thing I want to look at is Price. This is a really terrible model.

```{r}
# let's save a new df for plotting afterward
myvars <- c("Sales", "Price", "CompPrice")
quick.lm.df <- scaled.merged[myvars]

price.lm <- lm(Sales~Price, data=quick.lm.df)
price.lm.summary <- summary(price.lm)

print(price.lm.summary)
```

Here's what this looks like:

```{r}
quick.lm.df = quick.lm.df %>% 
  add_predictions(price.lm, var = 'prediction')

quick.lm.df %>%
  ggplot(aes(Price, Sales)) +
  geom_point() +
  geom_smooth(method = 'lm', formula = y ~ x) +
  ggtitle('Sales vs Price Linear Model')
```

Just for fun let's explore what a 'better' model might look like. I'm not going into fit details here.


```{r}
quick.lm.df %>%
  ggplot(aes(Price, Sales)) +
  geom_point() +
  geom_smooth(method = 'gam', formula = y ~ s(x)) +
  ggtitle('Sales vs Price GAM Linear Model')
```

It's not any better with CompPrice.

```{r}
quick.lm.df %>%
  ggplot(aes(CompPrice, Sales)) +
  geom_point() +
  geom_smooth(method = 'lm', formula = y ~ x) +
  ggtitle('Sales vs Price Linear Model')
```


```{r}
quick.lm.df %>%
  ggplot(aes(CompPrice, Sales)) +
  geom_point() +
  geom_smooth(method = 'gam', formula = y ~ s(x)) +
  ggtitle('Sales vs CompPrice GAM Linear Model')
```


## Simple Linear Regression

This next section jumps into a variety of linear models.

For the model below, it appears that residuals are roughly symmetrical around 0. That's good. Note how close to zero most of the coefficient estimates are. Most variables, even if significant, have a weak relationship to Sales.

```{r}
simple.lm <- lm(Sales~., data=scaled.merged)
simple.summary <- summary(simple.lm)

print(simple.summary)
```

I want to look at the residuals. I don't see anything that gives me a strong reaction.

```{r}
par(mfrow=c(2,2))
plot(simple.lm)
```


## Linear Models Using Feature Selection

Let's do the same thing, but control the subsets using `leaps`.

```{r}
regfit.full=regsubsets(Sales~., data=scaled.merged, nvmax=5)
reg.summary=summary(regfit.full)

print(reg.summary)
```

Here we can compare a few different subsets. Again, no surprises.

```{r}
par(mfrow=c(1,2))
plot(regfit.full,scale="r2")
plot(regfit.full,scale="bic")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="adjr2")
```


We'll just take code straight from the example on Canvas...

```{r}
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
```


## Adding Interaction Terms
Here we define a new model with some interaction terms:
    a.  Income and Advertising
    b.  Income and CompPrice
    c.   Price and Age

The interaction terms have weak $\beta$ values, even though once appears to be statistically significant. 

```{r}
interaction.lm <- lm(Sales~. + Income*Advertising + Income*CompPrice + Price*Age, data=scaled.merged)
interaction.summary <- summary(interaction.lm)

print(interaction.summary)
```


We'll run the same model through the `leaps()` package in order to do some model selection (though, it's pretty obvious which features are helpful already).


```{r}
interaction.lm.subsets <- regsubsets(Sales~. + Income*Advertising + Income*CompPrice + Price*Age,
                                     data=scaled.merged, nvmax=5)

interaction.subsets.summary <- summary(interaction.lm.subsets)

print(interaction.subsets.summary)
```


```{r}
par(mfrow=c(1,2))
plot(interaction.lm.subsets,scale="r2")
plot(interaction.lm.subsets,scale="bic")
plot(interaction.lm.subsets,scale="Cp")
plot(interaction.lm.subsets,scale="adjr2")
```


## Variable Significance
Below we print the coefficients for the 5th model using the default model selection criteria. All coefficients are relatively small, as we would expect from the EDA above. This pretty much confirms what I would have guessed by looking at the data against sales. We still want to watch out for confounding between 'Price' and 'CompPrice.'

```{r}
coef(interaction.lm.subsets, 5)
```

## Second Interaction Model

First, drop columns unneeded from analysis, just to keep things clean.

```{r}
scaled.merged.slim <- scaled.merged[ , -which(names(scaled.merged) %in% c("US","Urban"))]
```

A hyper parameter we'd like to be consistent for all models. I tried a number of values here, but this one seemed fine for now. 

```{r}
nvmax <- 5
```

## Forward Selection:

```{r}
interaction.subset.fwd <- regsubsets(Sales~. + Income*Advertising + Income*CompPrice + Income*Age,
                                     data=scaled.merged.slim,
                                     nvmax = nvmax,
                                     method="forward")

fwd.subset.summary <- summary(interaction.subset.fwd)

coef(interaction.subset.fwd, 1:nvmax)
```

```{r}
par(mfrow=c(1,2))
plot(interaction.subset.fwd,scale="r2")
plot(interaction.subset.fwd,scale="bic")
plot(interaction.subset.fwd,scale="Cp")
plot(interaction.subset.fwd,scale="adjr2")
```


## Backward Selection:

This is really strange. I can't seem to find any documentation about this, but it appears that this model is actually the same as 'forward'. 

```{r}
interaction.subset.bk <- regsubsets(Sales~. + Income*Advertising + Income*CompPrice + Income*Age,
                                     data=scaled.merged.slim,
                                     nvmax = nvmax,
                                     method="backward")

bk.subset.summary <- summary(interaction.subset.bk)

coef(interaction.subset.bk, 1:nvmax)
```

```{r}
par(mfrow=c(1,2))
plot(interaction.subset.bk,scale="r2")
plot(interaction.subset.bk,scale="bic")
plot(interaction.subset.bk,scale="Cp")
plot(interaction.subset.bk,scale="adjr2")
```


## Exhaustive

Huh, now I get stumped. Exact same coefficients, exact same order as the previous two. Either this is a trick or I've done something wrong somewhere.

```{r}
interaction.subset.ex <- regsubsets(Sales~. + Income*Advertising + Income*CompPrice + Income*Age,
                                     data=scaled.merged.slim,
                                     nvmax = nvmax,
                                     method="exhaustive")

ex.subset.summary <- summary(interaction.subset.ex)
coef(interaction.subset.ex, 1:nvmax)
```

```{r}
par(mfrow=c(1,2))
plot(interaction.subset.ex,scale="r2")
plot(interaction.subset.ex,scale="bic")
plot(interaction.subset.ex,scale="Cp")
plot(interaction.subset.ex,scale="adjr2")
```


This is a list that may come in handy.

```{r}
model.list = list(list("Forward", interaction.subset.fwd, fwd.subset.summary),
                  list("Backward", interaction.subset.bk, bk.subset.summary),
                  list("Exhaustive", interaction.subset.ex, ex.subset.summary))
```

# Evaluation Metric Plotting

This isn't interesting, since all three models appear to be identical.

```{r}
par(mfrow=c(2,2))
plot(fwd.subset.summary$rss,xlab="Number of Variables (forward)",ylab="RSS",type="l")
plot(fwd.subset.summary$adjr2,xlab="Number of Variables (forward)",ylab="Adjusted RSq",type="l")
plot(fwd.subset.summary$cp,xlab="Number of Variables (forward)",ylab="Cp",type='l')
plot(fwd.subset.summary$bic,xlab="Number of Variables (forward)",ylab="BIC",type='l')
```

```{r}
par(mfrow=c(2,2))
plot(bk.subset.summary$rss,xlab="Number of Variables (backward)",ylab="RSS",type="l")
plot(bk.subset.summary$adjr2,xlab="Number of Variables (backward)",ylab="Adjusted RSq",type="l")
plot(bk.subset.summary$cp,xlab="Number of Variables (backward)",ylab="Cp",type='l')
plot(bk.subset.summary$bic,xlab="Number of Variables (backward)",ylab="BIC",type='l')
```

```{r}
par(mfrow=c(2,2))
plot(ex.subset.summary$rss,xlab="Number of Variables (exhaustive)",ylab="RSS",type="l")
plot(ex.subset.summary$adjr2,xlab="Number of Variables (exhaustive)",ylab="Adjusted RSq",type="l")
plot(ex.subset.summary$cp,xlab="Number of Variables (exhaustive)",ylab="Cp",type='l')
plot(ex.subset.summary$bic,xlab="Number of Variables (exhaustive)",ylab="BIC",type='l')
```

# Model Equations

Here we print the final equations for each model. Note, they are all the same.

```{r}

for (mod.obj in model.list) {
  mod.name <- mod.obj[[1]]
  best.bic <- min(mod.obj[[3]]$bic)
  mod.num <- which.min(mod.obj[[3]]$bic)
  mod.cc <- coef(mod.obj[[2]], mod.num)

  mod.equation.format <- paste("Y =", paste(round(mod.cc[1],2),
                      paste(round(mod.cc[-1],2),
                      names(mod.cc[-1]),
                      sep=" * ", collapse=" + "),
                      sep=" + "), "+ e")

  print(paste("Model Selection Method: ",mod.name))
  print(paste("Min BIC:", best.bic))
  print(paste("Model Number: ", mod.num))
  print(paste("Model Equation: ", mod.equation.format))
  print("")

}
```

Formally, we are looking for:
$$\hat{Y} = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \epsilon$$
Our model yields:
$$\hat{Y} = -0.27 + 1.27(ShelveLocGood) + 0.49(CompPrice) + -0.76(Price)$$
# Conclusions

Price, Shelf Location (good), and CompPrice seem to drive the variability and the predictive power of all models I tested. This would suggest a rather impulsive, price conscience customer that is easily influenced by how easy a car seat it is get off the shelf (if it is a physical shelf). Further, it seems that advertising has an effect on the customer's behavior. I would loved to have seen a safety rating feature in this data set.

Regularization was not very helpful, only a small number of variables meaningfully explained variance in the response variable. The model selection shows this clearly by selecting the same model no matter which method is chosen.

# Appendix A: PCA
This is what I would try (though, I think there is enough evidence to suggest that this data is synthetic.) PCA doesn't help us very much, but we'll see...

```{r}
scaled.nums <- dplyr::select_if(scaled.merged, is.numeric)
scaled.nums <- scaled.nums[,-1]
```


```{r}
pc_result = principal(scaled.nums, nfactors = 3)
print(pc_result$Vaccounted)
print(pc_result$weights)
```


```{r}
fa.diagram(pc_result)
```
```{r}
biplot(pc_result)
```

```{r}
pca.df <- cbind(pc_result$scores, scaled.merged[1:4])
pca.df.h1 <- one_hot(as.data.table(pca.df))
head(pca.df.h1)

```

Empirically I tried a GAM model and a SVM. These weren't impressive enough out of the box to show here. On to the next task...

```{r}
pca.df.h1 %>%
  ggplot(aes(x=RC1, y=Sales, color=ShelveLoc_Good)) +
  geom_point() +
  geom_smooth(method = 'gam', formula = y ~ s(x)) +
  ggtitle('Sales vs PC1 GAM Linear Model')
```

```{r}
pca.df.h1 %>%
  ggplot(aes(x=RC2, y=Sales, color=ShelveLoc_Good)) +
  geom_point() +
  geom_smooth(method = 'gam', formula = y ~ s(x)) +
  ggtitle('Sales vs PC2 GAM Linear Model')
```


